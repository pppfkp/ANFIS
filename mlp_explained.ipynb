{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podstawy sieci neuronowych\n",
    "Sieci neuronowe są podstawowymi narzędziami w uczeniu maszynowym, napędzającymi wiele najnowocześniejszych algorytmów i aplikacji w różnych dziedzinach, w tym w computer vision, nlp, robotyce i nie tylko.\n",
    "\n",
    "Sieć neuronowa składa się z połączonych ze sobą węzłów, zwanych neuronami, zorganizowanych w warstwy. Każdy neuron odbiera sygnały wejściowe, wykonuje na nich obliczenia przy użyciu funkcji aktywacji i generuje sygnał wyjściowy, który może być przekazywany do innych neuronów w sieci. Funkcja aktywacji określa wyjście neuronu, biorąc pod uwagę jego dane wejściowe. Funkcje te wprowadzają **nieliniowość** do sieci, umożliwiając jej uczenie się złożonych wzorców w danych.\n",
    "\n",
    "Sieć jest zazwyczaj podzielona na warstwy, zaczynając od warstwy wejściowej, w której wprowadzane są dane. Następnie znajdują się warstwy ukryte, w których wykonywane są obliczenia, a na końcu warstwa wyjściowa, w której podejmowane są decyzje.\n",
    "\n",
    "MLP są trenowane przy użyciu algorytmu wstecznej propagacji, który oblicza gradienty funkcji straty w odniesieniu do parametrów modelu i iteracyjnie aktualizuje parametry w celu zminimalizowania straty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieci neuronowe typu feedforward (FNN)\n",
    "Są to najprostsze sieci neuronowe, w których informacje przepływają w jednym kierunku, od wejścia do wyjścia. W architekturze sieci nie występują cykle ani pętle. Perceptrony wielowarstwowe (MLP) są rodzajem sieci neuronowej typu feedforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "Perceptron wielowarstwowy (MLP) to rodzaj sieci neuronowej typu feedforward składającej się z w pełni połączonych neuronów z nieliniowym rodzajem funkcji aktywacji. Jest szeroko stosowany do rozróżniania danych, które nie są liniowo rozdzielne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Nasz model MLP składa się z trzech warstw. Pierwsza warstwa przyjmuje dane wejściowe o wymiarze *input_dim* i przekształca je na wektor o wymiarze 64, a następnie używa funkcji aktywacyjnej ReLU. Druga warstwa przekształca dane z wymiaru 64 do 32, także z aktywacją ReLU. Ostatnia warstwa redukuje wymiar do 1, bez aktywacji, ponieważ będziemy używać modelu w zadaniu regresyjnym (tak aby było to zgodne  z naszym ANFISem). Jest to stosunkowo prosty model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
